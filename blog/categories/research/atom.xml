<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">

  <title><![CDATA[Category: research | #error NO_MONEY]]></title>
  <link href="http://enukane.github.io/blog/categories/research/atom.xml" rel="self"/>
  <link href="http://enukane.github.io/"/>
  <updated>2017-03-20T15:59:31+09:00</updated>
  <id>http://enukane.github.io/</id>
  <author>
    <name><![CDATA[n_kane]]></name>
    <email><![CDATA[enukane@glenda9.org]]></email>
  </author>
  <generator uri="http://octopress.org/">Octopress</generator>

  
  <entry>
    <title type="html"><![CDATA[システム系論文紹介 Advent Calendar 2016 - 時刻同期話]]></title>
    <link href="http://enukane.github.io/blog/2016/12/04/2016-system-paper-adv-calendar/"/>
    <updated>2016-12-04T10:58:41+09:00</updated>
    <id>http://enukane.github.io/blog/2016/12/04/2016-system-paper-adv-calendar</id>
    <content type="html"><![CDATA[<h1>論文紹介: “Globally Synchronized Time via Datacenter Networks”</h1>

<p>本記事は システム系論文紹介 Advent Calendar 2016の4日目, 12/04 のための記事です.</p>

<h2>はじめに</h2>

<p>4日目 n_kane の担当分では今年の ACM SIGCOMM 2016 より時刻同期話ということで以下の論文を取り上げます.</p>

<ul>
<li>タイトル: Globally Synchronized Time via Datacenter Networks

<ul>
<li>著者 Ki Suh Lee et al. (Cornell University)</li>
<li>出展: ACM SIGCOMM 2016, Session 5, Datacenters 1</li>
<li><a href="http://dl.acm.org/authorize?N19282">論文へのリンク</a></li>
<li><a href="http://conferences.sigcomm.org/sigcomm/2016/files/program/sigcomm/Session05-Paper01-Global-Ki.pdf">スライド資料へのリンク</a></li>
</ul>
</li>
</ul>


<p>セッション自体の括りはデータセンター, 内容としても DC 環境での分散システム向けの時刻同期をターゲットにしています. このあたりは個人的な興味ではなかったのですが, 最近時刻同期関連(GPS, NTP, PTP 等等)を勉強しようと思っておりました. その矢先にこの論文を見つけたため今回取り上げることにしました.</p>

<h2>対象としている問題</h2>

<p>NTPやPTPをベースにした時刻同期はもはや無くてはならないプロトコルですが, ナノ秒レベルでの時刻同期が必要な場合, 精度に非決定性があるというのが本論文で取り上げ解決策を提示している問題です.</p>

<p>NTPはマイクロ〜ミリ秒, PTPはサブナノ秒の精度で時刻(クロック)同期が可能なプロトコルです. これらのプロトコルでは細かい差異はあるにせよ共に, 2台の計算機の間でRTTを測定し これをもとに一方向遅延(OWD One Way Delay)と相互のクロックの差(Offset)を算出, 時刻やクロックを合わせ, クロック発振器がそれを維持し定期的に再計測を行うという方法で 同期を行います.</p>

<p>この流れの RTT 計測, クロックの維持, 再計測に非決定的な誤差を生む要因がある, と本論文では主張しています. NTP, PTPはともに UDP ベースのプロトコルでありネットワーク帯域を消費しているため 間の経路の経路, 輻輳, 機器類のバッファリングや送受信のスケジューリングによる影響を受けます. ベースとなるRTT計測は往路復路が同じであることを前提としていますが, これが常に満たされるとは限りません. クロックの維持も誤差要因となります. 原子時計と異なり一般的なx86の計算機に積まれたクロックはそれぞれに 一定の誤差を生じながら動くため, 時が進むにつれズレが発生します. この補正のためには頻繁な再計測・再同期を行う必要がありますが, あまりに同期対象や頻度が多い場合に時刻同期に帯域を取られてしまうといった問題があります.</p>

<h2>解決策</h2>

<p>この論文では時刻同期を Ethernet で直結された2台の計算機間において PHY レイヤで行うことで 前節の問題を解決しようとしています. キモはEthernet で接続された計算機間では「既にNIC同士のクロックが同期されている」という点です. Figure 2 にクロックドメインについての図が掲載されていますが, Ethernetのフレームを送受信するにあたって送信側と受信側は実質同じ回路になっており 送信側のクロックに併せて動作をしていると考えることができます.</p>

<p>このNICレベルでのクロック同期を, システムレベルでのクロック同期に利用するというのが 本論文で提示する手法になっています. この手法を用いたクロック同期として DTP (Datacenter Time Protocol) とそれを実装した 10Gbit Ethernet PHY を取り上げています.</p>

<h2>ポイント</h2>

<p>この論文で DTP の推しポイントとして主張されているのは以下の3点です.</p>

<ol>
<li>802.3プロトコルのハックによるオーバーヘッド実質0のプロトコル</li>
<li>ナノ秒レベルでの同期で誤差が予測可能</li>
<li>スイッチを用いたスケーラブルなクロック同期が可能</li>
</ol>


<h3>1. オーバーヘッド実質0のプロトコル</h3>

<p>NTPやPTPと同じく DTP も RTT の計測からの一方向遅延の算出を基本としています. 最終的なクロック合わせをオフセットの計算ではなく「一番速いやつに合わせる」 というアルゴリズムの違いはありますが, やっていることはあまり替わりません.</p>

<p>大きな違いは先にも述べたとおり DTP では Ethernet の PHY レイヤで伝送を行う点です. 具体的には PHY の PCS (Protocol Control Sublayer) のスクランブル/デスクランブル化の直前に, 処理を差し込むことでこのレイヤで伝送されているコントロールブロックに載せて DTPのデータを送受信します.</p>

<p>このレイヤでは実際のデータ(Ethernetフレーム)転送の間にリンク維持やエラー通知を目的とした コントロールブロックの送受信が行われています. このうちDTPが有効なリンクでは Idle キャラクタの部分に DTP のデータを載せ送受信することとしています. PCSの上位レイヤには Idle キャラクタを正しく戻してやることで, Ethernetのデータ転送の帯域を実質的に 使うこと無くDTPのやりとりを行うことが可能となっています.</p>

<p>この方法の利点として Ethernet の伝送を邪魔しないこと, 高頻度にクロック同期が可能であることが挙げられます. Idleキャラクタのコントロールブロックは Ethernet フレームが流れる時はその前後に, 何も流れていない時は継続的に差し込まれるため Ethernet の帯域を消費しません. いわゆる10GbE, 100GbEといった速度はこの制御系の通信を除いたものであるためです. このコントロールブロックは輻輳している場合でも 1280〜7680ns の間隔で挿入が可能です. ワーストケースでも数usの周期でクロック同期を回すことが可能であるため, 精度の維持</p>

<h3>2. ナノ秒レベルで誤差予測可能なクロック同期</h3>

<p>DTP では同期誤差が「4T」に決定的(deterministic)に収まるように設計されています. ここで T は最も速いクロックの周期であり, 10Gbit Ethernetの場合は T = 1 / f = 1 / 156.25MHz = 6.4 nsとなるため, 25.6ns内に収まることになります.</p>

<p>この誤差予測が可能なのは PHY レイヤで同期しているためソフトウェアスタックが介在しないこと, 直結されているため間に何も入らないことにより誤差導入要素が(ほぼ)無いためです. 遅延要因としてはケーブル上の伝搬遅延やエンドポイントでの処理遅延が存在しますが これらは動的には変化しないと仮定を置くことができ, 事前に予測が可能です 一部 Clock Domaing Crossing, 相手のTXに乗ったクロックと自分のTXのクロック間の 遅延を解決するのにランダム性のある誤差が生じますがこれもどちらか速いほうの1クロック内に 収まるということのようです.</p>

<p>複数のPHYを計算機に挿すことでPTPのBoundary Clockのようにネットワークを跨がって時刻を 同期することも可能です. この場合でも「4TD + 8T」に誤差が収まるとしています. ここで D はホップ数を挿します. スモールワールド現象に則って6ホップ経由すればデータセンターの 全ての計算機にリーチできると仮定すると, どの計算機の間でも 153.6ns 以内に誤差が収まる クロック同期が可能となります.</p>

<h3>3. スケーラブルな時刻同期</h3>

<p>DTP は Ethernet が直結された2台間で行うことが基本ですが, スイッチがDTPをサポートすることで 2台以上のクロック同期が可能となっています. 前提としてスイッチの全ポートが同じクロックを共有するスイッチチップにより制御されていることが 必須にはなりますが, これを基軸として全ポートと DTP のやりとりを行うことで現在の最速クロックに 合わせるという動作が可能です.</p>

<h2>評価</h2>

<p>有効性の評価としてこの論文では DTP が PTP と比べて非決定性を抑制できていることを確認しています. PTPでは負荷の状況をなし, 中度, 重度と変えたときに300ns程度内, 25us程度内, 100us程度内と 誤差が大きくなっていきまた定常的にもブレが大きいことが見て取れます. 一方の DTP ではMTU 1500バイトの通常のEthernetややジャンボフレームの場合に負荷を掛けても ワーストケースで 4T = 25.6 ns内に常に収まっていることが確認できます.</p>

<h2>まとめとおわりに</h2>

<p>ここでは Ethernet の PHY レイヤを用いたクロック同期手法 DTP (Datacenter Time Protocol) についての論文を取り上げました. 時刻同期というよりはクロック同期であり, PTPと比してさらにハードウェアのサポートが必要であること, 同期ピア間でケーブルの直結が必要であることなど制約がより強い手法ではあります. ただし誤差の予測がネットワークを跨いでも可能であり必ずしも局所的にしか使えないといったものでもないようです.</p>

<p>スイッチの実装はまだ構想段階のようなので論文中の前提がフィールドで適用できるかどうかや, DTP を実装する NIC PHY のコストや性能への実際の影響についてはより調査や実験が必要と感じました. 個人的には IEEE 802.11ae の PCS をある意味ハックしているのが面白く感じました. また REFERENCES に時刻同期関連の一通りが並んでいるので大変にありがたい文章です.</p>

<h2>後付け: 他の候補</h2>

<p>その他, 今回紹介しようと思った候補としては以下に挙げるものがありました.</p>

<ul>
<li>ACM SIGCOMM 2016: Inter-Technology Backscatter: Towards Internet Connectivity for Implanted Devices, Iyer et al (University of Washington)</li>
<li>ACM SIGCOMM 2016: Evolve or Die: High-Availability Design Principles Drawn from Google’s Network Infrastructure, Govindan et al (Google/USC)</li>
</ul>

]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[SIGCOMM2015: Large-scale Measurements of Wireless Network Behavior を読む]]></title>
    <link href="http://enukane.github.io/blog/2015/08/28/sigcomm2015-large-scale-wlan-net/"/>
    <updated>2015-08-28T09:10:56+09:00</updated>
    <id>http://enukane.github.io/blog/2015/08/28/sigcomm2015-large-scale-wlan-net</id>
    <content type="html"><![CDATA[<h2>概要</h2>

<p>リンク: <a href="http://conferences.sigcomm.org/sigcomm/2015/pdf/papers/p153.pdf">http://conferences.sigcomm.org/sigcomm/2015/pdf/papers/p153.pdf</a></p>

<ul>
<li>SIGCOMM &lsquo;15で発表された論文。Cisco Merakiの人たちが著者。</li>
<li>Merakiはいわゆるクラウド型無線LANコントローラとAPのセットのサービス。</li>
<li>自社のサービスを展開している中で収集されたデータのオーバービュー的な文章。</li>
</ul>


<h3>読んだモチベーション</h3>

<ul>
<li>Wireless Network という題に惹かれて</li>
<li>お仕事で似た様なことやってるため</li>
</ul>


<h2>内容</h2>

<p><a href="http://lafrenze.hatenablog.com/entry/2015/08/04/120205">落合先生フォーマット</a>に従って並べてみる</p>

<h3>どんなもの？</h3>

<ul>
<li>Merakiのサービスを提供するに当たって収集したデータの分析を行った

<ul>
<li>2万台のMeraki AP, 2万個のネットワーク, 5百万クライアント/week の規模</li>
<li>これらのネットワークにぶら下がる数百万のデバイスの様々な情報が自社のDBに溜まっている

<ul>
<li>ネットワークに所属するデバイスたち: AP, クライアント端末(ノートPC, スマホ e.t.c), スイッチ, ルータ など</li>
<li>情報: 接続断のイベント, 通信先 など</li>
<li>Meraki(のサービス側)は, いわゆる無線LANコントローラなのでこれらの情報を常にpolling/収集している</li>
</ul>
</li>
</ul>
</li>
<li>これだけ規模が大きく, 期間も長いと&#8221;無線LANネットワークのトレンド&#8221;的なものが見えてくる

<ul>
<li>電波的な傾向

<ul>
<li>他のAP or 802.11以外の干渉</li>
</ul>
</li>
<li>規格上の傾向

<ul>
<li>2.4 or 5GHz帯への偏り</li>
<li>802.11g, a, n, acの偏り(利用率の変化)</li>
<li>チャネル幅の遷移</li>
<li>ストリーム数の遷移</li>
</ul>
</li>
<li>クライアントの傾向

<ul>
<li>OSのバリエーション</li>
<li>信号強度</li>
</ul>
</li>
<li>ネットワークの傾向

<ul>
<li>いわゆる普通のトラフィック分析</li>
<li>アプリケーション利用率</li>
</ul>
</li>
</ul>
</li>
</ul>


<h3>先行研究と比べて何がすごい？何が違う？</h3>

<ul>
<li>まず数

<ul>
<li>Gemberら(当該研究者が所属する大学ネットワークや施設のネットワークが対象)、Rodrigら(カンファレンスネットワークが対象)等、干渉の計測やネットワークトラフィックの分析を行った研究は存在する。この論文では、さらに大規模なユーザベースでの分析を行っている。</li>
<li>Hotspotを対象にそこそこの規模で行った例としては Ghoshら(243000デバイス)、Google (500AP, 30000クライアント)等があるがこれらはあくまでHotspot向けであり、この論文(およびMerakiのサービス)が対象としているオフィス/キャンパスユースではない</li>
</ul>
</li>
<li>計測期間の長さ (とそこから来るデータ量)

<ul>
<li>おもに他の研究ではショットないし1年内程度のスパンで行ったものが主

<ul>
<li>短いスパンであれば、個々の技術・計測に関する分析を行った既存研究は多種存在する</li>
<li>干渉の定性的分析や規格の技術的妥当性の検証という観点で</li>
</ul>
</li>
<li>この論文では5年程度のスパンで見ている

<ul>
<li>&ldquo;トレンド&#8221;という点に着目できる内容になっている</li>
<li>さらに取得した、サービスの継続にあたって重要な無線LANインフラ(!=無線LAN only)の情報を分析対象としている</li>
</ul>
</li>
</ul>
</li>
</ul>


<h3>技術の手法とかキモはどこ？</h3>

<ul>
<li>サービスのプロダクション環境に当該情報収集系を組み込んだところ

<ul>
<li>いわゆる&#8221;実用&#8221;の場でのデータが取れる</li>
<li>お客さんのバリエーション == &ldquo;実用”の度合い に直結する</li>
</ul>
</li>
<li>観点として「サービスの継続性」のための情報収集に特化しているところ

<ul>
<li>個々の規格・技術の妥当性検証というよりは、その現れ方・利用され方の観測が主</li>
</ul>
</li>
</ul>


<h3>どうやって有効だと検証した？</h3>

<ul>
<li>Evaluationとして有効性を示す文章はない</li>
<li>強いて言えば実環境でこんな感じのデータが取れたぜ！という報告に近い?</li>
</ul>


<h3>議論はある？</h3>

<ul>
<li>(TBD)</li>
</ul>


<h3>次に読むべき論文は？</h3>

<ul>
<li>GamberらのとGoogleのやつ？

<ul>
<li><ol type="a">
<li>Gember, A. Anand, and A. Akella. A comparative study of handheld and non-handheld traffic in campus wi-fi networks.</li>
</ol>
</li>
<li><ol type="a">
<li>Afanasyev, T. Chen, G. M. Voelker, and A. C. Snoeren. Usage patterns in an urban wifi network</li>
</ol>
</li>
</ul>
</li>
<li>Senらの&#8221;Cspy: finding the best quality channel without probing&#8221;も気になる

<ul>
<li><ol type="a">
<li>Sen, B. Radunovic, J. Lee, and K.-H. Kim. Cspy: finding the best quality channel without probing.</li>
</ol>
</li>
</ul>
</li>
</ul>

]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[システム系論文紹介 Advent Calendar 2014]]></title>
    <link href="http://enukane.github.io/blog/2014/12/12/syspaper-adventcalendar-2014/"/>
    <updated>2014-12-12T09:58:24+09:00</updated>
    <id>http://enukane.github.io/blog/2014/12/12/syspaper-adventcalendar-2014</id>
    <content type="html"><![CDATA[<p>本記事は、<a href="http://www.adventar.org/calendars/440">システム系論文紹介 Advent Calendar 2014</a> 12/11 のための記事です</p>

<h1>はじめに</h1>

<p>本エントリでは、NSDI&#8217;14 にて発表のあった &ldquo;NetVM&rdquo; について紹介します。</p>

<h1>&ldquo;NetVM: High Performance and Flexible Networking Using Virtualization on Commodity Platforms&rdquo;</h1>

<h2>論文の概要</h2>

<ul>
<li>タイトル: <a href="https://www.usenix.org/conference/nsdi14/technical-sessions/presentation/hwang"><em>&ldquo;NetVM: High Performance and Flexible Networking Using Virtualization on Commodity Platforms&rdquo;</em></a></li>
<li>著者: Jinho Hwang et al.</li>
<li>出典: 11th USENIX Symposium on Networked Systems Design and Implementation (NSDI&#8217;14)</li>
</ul>


<p>この論文では表題にも登場する&#8221;Commodity Platforms&#8221;いわゆる普通の(?) x86マシンを使った、高度かつ高速なネットワークサービスを実現するための基盤として、&#8221;NetVM&#8221; というプラットフォームを提案&amp;実装しています。これまでお高い専用のハードウェアでなければ実用に足る性能が出せなかったファイヤーウォールやルーティング、ロードバランサといったネットワークに必要な諸機能。これらをソフトウェアだけで実現し、いわゆる<a href="http://www.etsi.org/technologies-clusters/technologies/nfv">NFV</a> 的なものの基盤として使える様にするといったことを目的としています。特にこの論文では、ハードウェアアプライアンスに負けない性能(==ワイヤーレート)を目指しつつ、ソフトウェアとしての展開の容易さや機能実現に対する柔軟性といったネットワークサービスインフラにおける利便性を実現することを主眼としています。</p>

<h1>ここがひと味違うよNetVM</h1>

<p>ここではNetVMを支える技術、として論文中よりNetVMにおける「こだわり」ポイントをいくつか紹介します。</p>

<h2>Virtualization: 仮想化前提だよ</h2>

<h3>NetVMにおける仮想マシン: 1コンポーネント == 1VM</h3>

<p>NetVMにおける一つの特徴として、仮想化環境によるネットワークサービスの実現を特に意識している点が上げられます。
いわゆるハコモノを用いる一般的なネットワークでは用途に応じて様々なハコを組み合わせてネットワークを構成します。
これにはたとえばスイッチやルータ、ファイヤーウォールなどといったコンポーネントに設定を投入し、間を
LANケーブルで敷設するイメージでしょうか。
ブロードバンドルータなど一つのハコにまとめている場合もありますが、より大規模かつ高速な処理を要求される場合は
専用のアプライアンスを持ってくることが一般的でしょう。</p>

<p>NetVMでは、これらの物理的なハコとしての形を取っ払ってKVM上の仮想マシンを新たな仮想的なハコとしてそこに機能を押し込めています。
これによりソフトウェアの柔軟性(デプロイや変更の自動化、大規模化 etc)といった利点をネットワークの構成に導入しています。</p>

<p>それぞれのVMはハコモノとして、これまで行ってきたのと同じようにパケットの入力とそれに基づく出力の決定(宛先の変更やパケットの書き換えなど)を行います。
もちろん仮想マシンに押し込めたところで物理配線が消える訳ではないのでそれらは依然として存在しますが、このハコモノの間を埋めるLANケーブル
(とスイッチング)の役割の一部をNetVMではハイパバイザでも提供します。NetVMではこのホストで動く部分をNetVM Core, NetVM Managerと呼称しています。
ネットワークコンポーネント間の結線にあたるこの部分もソフトウェアとして抱き込むことで、ハコモノを仮想化したのと同様の利点をネットワーキングにも導入しています。</p>

<h2>VMという区分によるセキュリティ</h2>

<p>一つのハコモノを一つのVMに押し込めて動かすことはセキュリティ面でも利点があります。
NFV的な、一つの物理ハードウェア上に複数のネットワーク、複数のお客さん環境が載っている場合は
それぞれの間がまぜこぜにならないことが肝要です。
KVMが既にVMという区分で提供しているアイソレーションの仕組みに乗っかることでNetVMでは
各ネットワークコンポーネントのセキュリティを担保しています。</p>

<p>加えて必要なのは、それぞれのネットワーク間を間違って繋がないことです。
ここはホストOSつまりNetVM CoreやNetVM Managerの領分になりますが、このためにVMのグルーピングができるようになっています。
これにより、あるVM群は特定のユーザのもの、また別のVM群はそっちのユーザのものといった形で内部的に分離する事が可能です。</p>

<p>なおあくまで内部的な区分けにつき、外に出たフレームは対象とはなりません。このため実際にNFVっぽいことの基盤として使うにはその部分でVLANかますなりトンネリングするなりでネットワーク的に区分けすることが必要だったり？また、一つの物理的なハードウェア上での組み合わせを主眼にしているためか外に出ないといけないような場合の想定があまりなされていない感じもあります。</p>

<h2>High Performance: パケット処理性能↑</h2>

<p>ハードウェアなアプライアンス箱に性能で勝つ、というのを第一目標においているためかチューニングやテクニックの記述に論文のそこそこの部分を割いているためその一部をここでは解説します。</p>

<h3>DPDKで User-land Packet Processing</h3>

<p>いわゆる「ポーリングモード」+ 「ユーザランドにおけるパケット処理」+「バッチ処理」による高速化は、Intel DPDKのリリースやnetmap、あとちょっとマニアックなところだとrump(はどうなんだろう&hellip;?)などの登場によりもはや当たり前のテクニックとなってきました。特に&#8221;なんらかのアプリケーション&#8221;に特化した通信を行いたい場合は、これまで先人がせっせと築き上げてきたOSのレイヤやらコンテキストスイッチングによるオーバヘッドをぶち抜けるため非常に高速にパケットを処理することが可能です。</p>

<p>これらの仕組みはDPDKでもnetmapでも、基本的には直接ハードウェア(NIC)を触ってる人のみへの恩恵です。一方、NetVMではベースにDPDKを用いることでこれらの技術を利用しつつ、ネットワークサービスのコンポーネントたるVMの中のアプリケーションに対しても同様の利点を提供しています。</p>

<h3>システム全体でのパケットバッファ共有</h3>

<p>DPDKやnetmapは基本的に、NICとネットワークアプリケーションとで送受信リングとパケットバッファを共有することでコピーコストの削減を行っています。
NetVMでもDPDKを利用しているため、ホストOSとNICとの間でこれはすでになされていますがこの構造をさらに拡張し、VMにたいしても
同様の処理ができるように巨大な共有メモリ領域を用意し、ダミーのvirtual PCIデバイス経由でVMに対してこれを晒しています。
これによりNIC-ホストOS-VMの三者間でのZero Copyを実現しています。
このZero CopyはVM-VM間でも有効であり、ホストOS(NetVM Core)を通してのコンポーネント間のパケットのやりとりにあたっても
同様の恩恵を得ることができます。これは、ホスト-VM間のRx/Txリングがそれぞれ独立である一方で&#8221;同じネットワーク&#8221;間では
パケットバッファが共有されているためです。なお先に述べたネットワーク間のアイソレーション機能(グルーピング)は
このパケットバッファの共有範囲を分けることでも実現されています。</p>

<h3>CPU特性の活用</h3>

<p>バッファ共有によるZero Copyの拡張に加えて、マルチコアシステムを前提として以下の工夫がなされています。</p>

<ul>
<li>Lockless なシステムデザイン</li>
<li>NUMA-awareなデザイン</li>
</ul>


<p>netmapでも同様だったかと思いますが(マルチコアの場合はどうだったっけ&hellip;)、共有のパケットバッファの操作にあたりNetVMではロックを用いません。
あるキュー(Rx/Tx)は同時にホスト内またはゲストから操作しないように構成されます(できないのではなくやらない)。
ある一つのキューに対してロックが必要なケースは以下の場合です。</p>

<ol>
<li>ホストOS側でそれぞれのコアが同じキューを触ることがある場合</li>
<li>ゲストOSとホストOSが同じキューを触ることがある場合</li>
</ol>


<p>これらを避けるために、NetVMではコア間で触るキューを分けるようになっています。
まずNetVMのホストOS側ではコアごとにNICおよびVMとのキューを分けるようになっています。
これにより1の、同じキューを別々のコア(上で動作するスレッド)が触ることがないようにしています。
次に、ゲストOS(VM)側のスレッドとホストOS側のスレッドが同じキューを操作するシチュエーション(2)ですが、
これも1と同様にあるゲストOS(VM)上のスレッドが動くCPUを限定することで対処しています。
このゲスト上のスレッドも、同様に自分のCPUに割り当てられたキューしか触れないようにすることで実質的に
いわゆるキューを挟んだconsumer-producerにおいてRX側では「ホストOSのスレッドとしてキューにpush」と
「VM上のスレッドとしてキューからpop」が一つのCPUで完結することになります(これらは1つのCPU上では同時に実行されないためロックもいらない)。
このあたりの実装にはKVMのvCPU周りの管理機構をいじくって実現したとのこと。</p>

<p>NetVMではNUMAなマルチコアシステムにおけるメモリの距離とキャッシュの扱いについても考慮が入っています。
基本的なデザインとして、NetVMでは自分のコアに距離的に近いメモリ領域以外は触らないように構成されています
(遠いメモリにアクセスするとキャッシュは汚れるは遠いわで良くないよというお話はここでは割愛)。
これは上記の「キューごとのコア」という構造をベースに作られます。
まずNetVMではそれぞれのコアごとに触るメモリ領域を分けます (総メモリ量/コア数 == 1つのコアが触るメモリ領域)。
そしてその各々のメモリ領域に「キューごとのコア」が作られ、ゲスト(VM)と共有されます。
先に述べたように、Locklessデザインの制約によりあるキューはそれぞれ対応するCPUからしかアクセスされないため、
あるCPU(とその上で動くホスト/ゲスト上のスレッド)は常に自分から近いメモリ領域のみを触る様になります。
これは全てのホスト-ゲスト間のみならず、ゲスト-ゲスト間でのパケットのやりとりでも有効であり、最終的に
NICにパケットを引き渡すまでlocalityを維持したまま通信ができるようになります。</p>

<h1>終わりに</h1>

<p>ここではNSDI&#8217;14より&#8221;NetVM&#8221;の紹介をしました。こういったNFV的なプラットフォームはネットワークベンダやキャリアなど各社それぞれ様々な構想を出しており、それらの一例として作り方まで踏み込んだものとして面白い論文ですが、なによりもチューニングに結構命かけているあたりがこのNetVMの面白いところです。</p>

<h2>今回ボツになった他の候補者達</h2>

<p>最後に、今回読もうと思ってた論文の候補たちを並べておきます。</p>

<ol>
<li>Unikernels: Library Operating Systems for the Cloud (ATC&#8217;13)</li>
<li>HACK: Hierarchical ACKs for Efﬁcient Wireless Medium Utilization (ATC&#8217;14)</li>
<li>Hyper-Switch: A Scalable Software Virtual Switching Architecture (ATC&#8217;13)</li>
<li>Rekindling Network Protocol Innovation with User-Level Stacks (SIGCOMM Computer Communication Review)</li>
</ol>


<p>Unikernelsは@suma90h さんが既にシステム系論文輪読会で<a href="http://www.slideshare.net/suma_/reading-unikernels">お読みになってた</a> のでパス。HACKは、無線LAN系 &amp; 今年のATCでのBest Paperということもあって気になっているのですが別枠で読むことに。3や4は、NetVMと同じく&#8221;えすでーえぬ&#8221;とか&#8221;えぬえふぶい&#8221;といった類いに属しそうなのと、先日のIIJ-II セミナーで&#8221;ClickOS and the Art of Network Function Virtualization&#8221;のお話があったので関連文献として上げてみました。</p>
]]></content>
  </entry>
  
</feed>
