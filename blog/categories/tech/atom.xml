<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">

  <title><![CDATA[Category: Tech | #error NO_MONEY]]></title>
  <link href="http://enukane.github.io/blog/categories/tech/atom.xml" rel="self"/>
  <link href="http://enukane.github.io/"/>
  <updated>2015-02-13T22:46:04+09:00</updated>
  <id>http://enukane.github.io/</id>
  <author>
    <name><![CDATA[n_kane]]></name>
    <email><![CDATA[enukane@glenda9.org]]></email>
  </author>
  <generator uri="http://octopress.org/">Octopress</generator>

  
  <entry>
    <title type="html"><![CDATA[2015-02-12 DS-Liteいれてみた]]></title>
    <link href="http://enukane.github.io/blog/2015/02/12/2015-02-12-dslite/"/>
    <updated>2015-02-12T23:50:42+09:00</updated>
    <id>http://enukane.github.io/blog/2015/02/12/2015-02-12-dslite</id>
    <content type="html"><![CDATA[<h2>追記</h2>

<ul>
<li>2015/02/13 「速度計測その2」を追加</li>
</ul>


<h2>はじめに</h2>

<p>新居でのフレッツv6オプションおよび IIJmio FiberAccess/NF の準備ができたので
<a href="https://www.iijmio.jp/guide/outline/ipv6/ipv6_access/dslite/">DS-Lite</a>で
IPv4での疎通性を確保してみた.
が, ちょいと速度面で問題?があったのでメモ.</p>

<h2>設定</h2>

<p>設定は以下を参照. フィルタ周りの設定やIPv4 PPPoe周りを継ぎ足してほぼそのままで導入.</p>

<ul>
<li><a href="http://www.seil.jp/blog/ds-lite">SEIL/x86でDS-Lite</a></li>
</ul>


<h2>動作確認</h2>

<p>tracerouteを取ってみるとこんな感じに transix.jp を通って
IPv4インターネットに出て行けてることが確認できたら完了.
<img src="/images/traceroute-dslite.png" alt="traceroute-dslite" /></p>

<p>なお普通にIPv4 PPPoE経由でいくとこんな感じになる.
<img src="/images/traceroute-iijmio.png" alt="traceroute-iijmio" /></p>

<h2>速度計測</h2>

<p>問題は速度で, 以下の様な三者間で速度を測ってみるとどうにも DS-Lite 経由の iperf の結果が芳しくない.</p>

<p><img src="/images/dslitevspppoe.png" alt="dslitevspppoe" /></p>

<table>
<thead>
<tr>
<th style="text-align:left;">経由       </th>
<th style="text-align:left;">ISPまたは事業者    </th>
<th style="text-align:center;">対戦表</th>
<th style="text-align:right;">速度(Mbps)</th>
<th style="text-align:left;">備考             </th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:left;">???        </td>
<td style="text-align:left;"> sakura internet?  </td>
<td style="text-align:center;">1     </td>
<td style="text-align:right;">87.2      </td>
<td style="text-align:left;">VPS側の速度計測用</td>
</tr>
<tr>
<td style="text-align:left;">DS-Lite    </td>
<td style="text-align:left;"> Multifeed(transix)</td>
<td style="text-align:center;">2     </td>
<td style="text-align:right;">46.5      </td>
<td style="text-align:left;">                 </td>
</tr>
<tr>
<td style="text-align:left;">DS-Lite    </td>
<td style="text-align:left;"> Multifeed(transix)</td>
<td style="text-align:center;">3     </td>
<td style="text-align:right;">43.5      </td>
<td style="text-align:left;">                 </td>
</tr>
<tr>
<td style="text-align:left;">IPv4 PPPoE </td>
<td style="text-align:left;"> IIJmio            </td>
<td style="text-align:center;">2     </td>
<td style="text-align:right;">87.9      </td>
<td style="text-align:left;">                 </td>
</tr>
<tr>
<td style="text-align:left;">IPv4 PPPoE </td>
<td style="text-align:left;"> IIJmio            </td>
<td style="text-align:center;">3     </td>
<td style="text-align:right;">73.7      </td>
<td style="text-align:left;">                 </td>
</tr>
<tr>
<td style="text-align:left;">IPv4 PPPoE </td>
<td style="text-align:left;"> Interlink         </td>
<td style="text-align:center;">2     </td>
<td style="text-align:right;">88.6      </td>
<td style="text-align:left;">                 </td>
</tr>
<tr>
<td style="text-align:left;">IPv4 PPPoE </td>
<td style="text-align:left;"> Interlink         </td>
<td style="text-align:center;">3     </td>
<td style="text-align:right;">69.4      </td>
<td style="text-align:left;">                 </td>
</tr>
</tbody>
</table>


<ul>
<li>なお上流はフレッツ光マンションタイプ・ミニ(VDSL方式)なので,最大100Mbps</li>
<li>ルータにはSEIL/X1を利用</li>
<li>常にsakura VPS側がiperf server

<ul>
<li>1の場合は, 東京第二側をserverに</li>
</ul>
</li>
<li>めんどうなので一発勝負. 5回平均や最大値採取はせず</li>
</ul>


<p>おおよそIIJmio側もInterlink側も70〜80Mbps程度は出ているように見受けられる.
一方, DS-Liteを通すとなぜか40〜50Mbpsに下がる模様.</p>

<p>さて, なんででしょう.</p>

<h2>速度計測その2</h2>

<p>上記速度計測では, LAN内からはiperf clientとしてしか通信していなかったので,
ここではiperf3 (ver 3.0.11)のReverseモードを使って, 上り下り両方を測ってみる.
なお, 上記速度計測とは以下の違いがある.</p>

<ul>
<li>今度は MacBookAir + Thunderbolt Ethernet Adapterをiperf clientとして利用

<ul>
<li>若干速くなってる</li>
</ul>
</li>
<li>IIJmioとinterlinkには速度差はあまりないのでinterlink側経由の速度は省略</li>
</ul>


<table>
<thead>
<tr>
<th style="text-align:left;">経由       </th>
<th style="text-align:left;">ISPまたは事業者    </th>
<th style="text-align:center;">対戦表</th>
<th style="text-align:center;">向き      </th>
<th style="text-align:right;">速度(Mbps): sender</th>
<th style="text-align:right;">速度(Mbps): receiver</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:left;">DS-Lite    </td>
<td style="text-align:left;"> Multifeed(transix)</td>
<td style="text-align:center;">2     </td>
<td style="text-align:center;"> UP       </td>
<td style="text-align:right;"><strong>51.5</strong>          </td>
<td style="text-align:right;"><strong>51.2</strong>            </td>
</tr>
<tr>
<td style="text-align:left;">DS-Lite    </td>
<td style="text-align:left;"> Multifeed(transix)</td>
<td style="text-align:center;">2     </td>
<td style="text-align:center;"> DOWN     </td>
<td style="text-align:right;">90.4              </td>
<td style="text-align:right;">89.9                </td>
</tr>
<tr>
<td style="text-align:left;">DS-Lite    </td>
<td style="text-align:left;"> Multifeed(transix)</td>
<td style="text-align:center;">3     </td>
<td style="text-align:center;"> UP       </td>
<td style="text-align:right;"><strong>53.5</strong>          </td>
<td style="text-align:right;"><strong>53.1</strong>            </td>
</tr>
<tr>
<td style="text-align:left;">DS-Lite    </td>
<td style="text-align:left;"> Multifeed(transix)</td>
<td style="text-align:center;">3     </td>
<td style="text-align:center;"> DOWN     </td>
<td style="text-align:right;">90.9              </td>
<td style="text-align:right;">90.4                </td>
</tr>
<tr>
<td style="text-align:left;">IPv4 PPPoE </td>
<td style="text-align:left;"> IIJmio            </td>
<td style="text-align:center;">2     </td>
<td style="text-align:center;"> UP       </td>
<td style="text-align:right;">93.8              </td>
<td style="text-align:right;">93.8                </td>
</tr>
<tr>
<td style="text-align:left;">IPv4 PPPoE </td>
<td style="text-align:left;"> IIJmio            </td>
<td style="text-align:center;">2     </td>
<td style="text-align:center;"> DOWN     </td>
<td style="text-align:right;">94.1              </td>
<td style="text-align:right;">93.4                </td>
</tr>
<tr>
<td style="text-align:left;">IPv4 PPPoE </td>
<td style="text-align:left;"> IIJmio            </td>
<td style="text-align:center;">3     </td>
<td style="text-align:center;"> UP       </td>
<td style="text-align:right;">87.3              </td>
<td style="text-align:right;">86.3                </td>
</tr>
<tr>
<td style="text-align:left;">IPv4 PPPoE </td>
<td style="text-align:left;"> IIJmio            </td>
<td style="text-align:center;">3     </td>
<td style="text-align:center;"> DOWN     </td>
<td style="text-align:right;">88.6              </td>
<td style="text-align:right;">87.5                </td>
</tr>
</tbody>
</table>


<ul>
<li>iperf3はクライアント側からサーバ側に向けてトラフィックを出す</li>
<li>&ldquo;-R&#8221;オプションで逆方向になる</li>
<li>このため, &ldquo;-R&#8221;なしがUP, &rdquo;-R&#8221;ありがDOWN</li>
</ul>


<p>DS-LiteのUP方向が妙に抑えられているようだ. DOWN方向は普通に速度が出ている.</p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[2015-01-28 Embulk-plugin-input-pcapng-files書いた]]></title>
    <link href="http://enukane.github.io/blog/2015/01/29/2015-01-28-embulk-playground/"/>
    <updated>2015-01-29T00:03:51+09:00</updated>
    <id>http://enukane.github.io/blog/2015/01/29/2015-01-28-embulk-playground</id>
    <content type="html"><![CDATA[<h3>追記 (2015/01/29 20:03)</h3>

<p>ver 0.0.2切った. ファイルのソート周りはまだ直してない.</p>

<p><a href="https://rubygems.org/gems/embulk-plugin-input-pcapng-files">https://rubygems.org/gems/embulk-plugin-input-pcapng-files</a></p>

<h3>追記 (2015/01/29 19:49)</h3>

<p>手元で動作確認してたらいつの間にか次のバージョンのembulkがリリースされてた.
おかしいぞ&hellip;さっきのpullreqで入れてもらったワークアラウンド試し始めたばっかりなのに&hellip;(もう不要になった)</p>

<p><a href="https://twitter.com/frsyuki/status/560750747608817665">https://twitter.com/frsyuki/status/560750747608817665</a></p>

<h3>追記 (2015/01/29 19:38)</h3>

<p>frsyuki先生から後ろ方のてきとーなメモに対する, 大変丁寧なコメントを頂けた. なるほどなるほど.</p>

<p><a href="https://gist.github.com/frsyuki/dcfb30690fd453542f45">https://gist.github.com/frsyuki/dcfb30690fd453542f45</a></p>

<p>ついでにいろいろと手直しして頂いて感謝感謝</p>

<ul>
<li><a href="https://github.com/enukane/embulk-plugin-input-pcapng-files/pull/1">https://github.com/enukane/embulk-plugin-input-pcapng-files/pull/1</a></li>
<li><a href="https://github.com/enukane/embulk-plugin-input-pcapng-files/pull/2">https://github.com/enukane/embulk-plugin-input-pcapng-files/pull/2</a></li>
</ul>


<h2>はじめに</h2>

<p>前々からやっているコミケその他イベントでの無線LAN解析(※)に良い感じに使えそうなので
pcapngファイルからの入力を取れる &ldquo;embulk-plugin-input-pcapng-files&rdquo; なるプラグインを書いてみました.</p>

<ul>
<li><a href="https://github.com/enukane/embulk-plugin-input-pcapng-files">enukane/embulk-plugin-input-pcapng-files</a></li>
<li><p><a href="https://rubygems.org/gems/embulk-plugin-input-pcapng-files">rubygems - embulk-plugin-input-pcapng-files</a></p></li>
<li><p>※</p>

<ul>
<li><a href="http://www.slideshare.net/enukane/comiket-space-29723016">http://www.slideshare.net/enukane/comiket-space-29723016</a></li>
<li><a href="http://www.slideshare.net/enukane/comiket-space-c86">http://www.slideshare.net/enukane/comiket-space-c86</a></li>
<li><a href="http://www.slideshare.net/enukane/c87-wifi-comiket-space">http://www.slideshare.net/enukane/c87-wifi-comiket-space</a></li>
</ul>
</li>
</ul>


<p>中身はまんまtshark呼んでるだけですが, 抽出条件をコンフィグに書けたり出力先を柔軟に変更可能だったりと
embulkのよさげなところを活かせそうなのが良い感じです.</p>

<p>今までは集めた多量のpcapngファイルを, ひとつひとつ真心()込めてスクリプトに食わせてたので
これを期に自動化の流れが造れるとベター.
またそこまで行かなくともこれまでに溜め込んだpcapngファイルの再掘り起こしが容易になるので
それだけでも良い感じです.</p>

<p>正直pcapngをこんな感じに触りたい人いない気がするので, 誰にも使ってもらえない感&hellip;</p>

<h3>使い方</h3>

<p>コンフィグファイルはこんな感じになります.
いわゆるguessはできないので手打ちで全部書く必要があります.</p>

<pre><code>exec: {}
in:
  type: pcapng_files
  paths: [/Users/enukane/Desktop/emtestpcap/, /tmp]
  threads: 3
  schema:
    - { name: frame.time_epoch, type: long }
    - { name: frame.len, type: long }
    - { name: wlan.ta, type: string }
    - { name: wlan.ra, type: string }
out: {type: stdout}
</code></pre>

<ul>
<li>typeにはpcapng_filesを指定します</li>
<li>pathsには, 処理したいpcapngファイルが入ったディレクトリを配列で指定します</li>
<li>threadsは, 並列度を指定します

<ul>
<li>上記paths内で見つかったpcapngファイルたちをこのthreadsの数に分配して並行に処理がなされる, はず</li>
<li>ちゃんと動いてるかは未確認&hellip;</li>
</ul>
</li>
<li>schemaにはpcapng中の抽出したいフィールド名(name)と変換先の型(type)を指定します

<ul>
<li>フィールド名は, wireshark/tsharkで -e オプションのfilterとして使っている名前が指定できます</li>
<li>型は今のところstringかlongのみ</li>
</ul>
</li>
</ul>


<p>これをpreviewコマンドで指定してみるとこんな感じ</p>

<pre><code>enukane@glenda-lairなう（´・ω・｀）つ ~/Sources/embulk-test % java -jar embulk.jar preview config.yml
+-------------------------------------------------+-----------------------+----------------+-------------------+-------------------+
|                                     path:string | frame.time_epoch:long | frame.len:long |    wlan.ta:string |    wlan.ra:string |
+-------------------------------------------------+-----------------------+----------------+-------------------+-------------------+
| /Users/enukane/Desktop/emtestpcap//test1.pcapng |         1,413,615,217 |             45 | c4:7d:4f:56:e5:19 | 20:c9:d0:d8:37:31 |
| /Users/enukane/Desktop/emtestpcap//test1.pcapng |         1,413,615,217 |             39 |                   | c4:7d:4f:56:e5:19 |
| /Users/enukane/Desktop/emtestpcap//test1.pcapng |         1,413,615,217 |             57 | 20:c9:d0:d8:37:31 | c4:7d:4f:56:e5:19 |
| /Users/enukane/Desktop/emtestpcap//test1.pcapng |         1,413,615,217 |             45 | 20:c9:d0:d8:37:31 | c4:7d:4f:56:e5:19 |
| /Users/enukane/Desktop/emtestpcap//test1.pcapng |         1,413,615,217 |             39 |                   | 20:c9:d0:d8:37:31 |
| /Users/enukane/Desktop/emtestpcap//test1.pcapng |         1,413,615,217 |            146 | 20:c9:d0:d8:37:31 | c4:7d:4f:56:e5:19 |
| /Users/enukane/Desktop/emtestpcap//test1.pcapng |         1,413,615,217 |             57 | c4:7d:4f:56:e5:19 | 20:c9:d0:d8:37:31 |
| /Users/enukane/Desktop/emtestpcap//test1.pcapng |         1,413,615,217 |            151 | c4:7d:4f:56:e5:1c | 33:33:00:01:00:03 |
| /Users/enukane/Desktop/emtestpcap//test1.pcapng |         1,413,615,217 |            131 | c4:7d:4f:56:e5:1c | 01:00:5e:00:00:fc |
| /Users/enukane/Desktop/emtestpcap//test1.pcapng |         1,413,615,217 |            283 | c4:7d:4f:56:e5:18 | ff:ff:ff:ff:ff:ff |
| /Users/enukane/Desktop/emtestpcap//test1.pcapng |         1,413,615,217 |             45 | c4:7d:4f:56:e5:19 | 20:c9:d0:d8:37:31 |
| /Users/enukane/Desktop/emtestpcap//test1.pcapng |         1,413,615,217 |             39 |                   | c4:7d:4f:56:e5:19 |
| /Users/enukane/Desktop/emtestpcap//test1.pcapng |         1,413,615,217 |            146 | c4:7d:4f:56:e5:19 | 20:c9:d0:d8:37:31 |
| /Users/enukane/Desktop/emtestpcap//test1.pcapng |         1,413,615,217 |             57 | 20:c9:d0:d8:37:31 | c4:7d:4f:56:e5:19 |
| /Users/enukane/Desktop/emtestpcap//test2.pcapng |         1,413,615,217 |             45 | c4:7d:4f:56:e5:19 | 20:c9:d0:d8:37:31 |
| /Users/enukane/Desktop/emtestpcap//test2.pcapng |         1,413,615,217 |             39 |                   | c4:7d:4f:56:e5:19 |
| /Users/enukane/Desktop/emtestpcap//test2.pcapng |         1,413,615,217 |             57 | 20:c9:d0:d8:37:31 | c4:7d:4f:56:e5:19 |
| /Users/enukane/Desktop/emtestpcap//test2.pcapng |         1,413,615,217 |             45 | 20:c9:d0:d8:37:31 | c4:7d:4f:56:e5:19 |
| /Users/enukane/Desktop/emtestpcap//test2.pcapng |         1,413,615,217 |             39 |                   | 20:c9:d0:d8:37:31 |
| /Users/enukane/Desktop/emtestpcap//test2.pcapng |         1,413,615,217 |            146 | 20:c9:d0:d8:37:31 | c4:7d:4f:56:e5:19 |
| /Users/enukane/Desktop/emtestpcap//test2.pcapng |         1,413,615,217 |             57 | c4:7d:4f:56:e5:19 | 20:c9:d0:d8:37:31 |
| /Users/enukane/Desktop/emtestpcap//test2.pcapng |         1,413,615,217 |            151 | c4:7d:4f:56:e5:1c | 33:33:00:01:00:03 |
| /Users/enukane/Desktop/emtestpcap//test2.pcapng |         1,413,615,217 |            131 | c4:7d:4f:56:e5:1c | 01:00:5e:00:00:fc |
| /Users/enukane/Desktop/emtestpcap//test2.pcapng |         1,413,615,217 |            283 | c4:7d:4f:56:e5:18 | ff:ff:ff:ff:ff:ff |
| /Users/enukane/Desktop/emtestpcap//test2.pcapng |         1,413,615,217 |             45 | c4:7d:4f:56:e5:19 | 20:c9:d0:d8:37:31 |
| /Users/enukane/Desktop/emtestpcap//test2.pcapng |         1,413,615,217 |             39 |                   | c4:7d:4f:56:e5:19 |
| /Users/enukane/Desktop/emtestpcap//test2.pcapng |         1,413,615,217 |            146 | c4:7d:4f:56:e5:19 | 20:c9:d0:d8:37:31 |
| /Users/enukane/Desktop/emtestpcap//test2.pcapng |         1,413,615,217 |             57 | 20:c9:d0:d8:37:31 | c4:7d:4f:56:e5:19 |
+-------------------------------------------------+-----------------------+----------------+-------------------+-------------------+
</code></pre>

<ul>
<li>previewでは部分的なタスクしか実行されません (ここではtask5.pcapngがぬけている)

<ul>
<li>runコマンドで出力される, &ldquo;次の状態のコンフィグ&#8221;も出力されません</li>
</ul>
</li>
<li>ただし上記のように良い感じに表っぽい出力がなされます</li>
</ul>


<p>実際にrunコマンドで走らせると以下の様になります</p>

<pre><code>enukane@glenda-lairなう（´・ω・｀）つ ~/Sources/embulk-test % java -jar embulk.jar run config.yml -o config.yml
2015-01-29 11:29:44,264 [INFO]: main:org.embulk.exec.LocalExecutor: Running 3 tasks using 8 local threads
2015-01-29 11:29:44,265 [INFO]: main:org.embulk.exec.LocalExecutor: {done:  0 / 3, running: 0}
/Users/enukane/Desktop/emtestpcap//test5.pcapng,1413615217,45,28:cf:e9:4d:bb:91,c4:7d:4f:56:e5:1d
/Users/enukane/Desktop/emtestpcap//test5.pcapng,1413615217,39,,28:cf:e9:4d:bb:91
/Users/enukane/Desktop/emtestpcap//test5.pcapng,1413615217,155,28:cf:e9:4d:bb:91,c4:7d:4f:56:e5:1d
(中略)
/Users/enukane/Desktop/emtestpcap//test1.pcapng,1413615217,283,c4:7d:4f:56:e5:18,ff:ff:ff:ff:ff:ff
/Users/enukane/Desktop/emtestpcap//test1.pcapng,1413615217,146,c4:7d:4f:56:e5:19,20:c9:d0:d8:37:31
/Users/enukane/Desktop/emtestpcap//test1.pcapng,1413615217,57,20:c9:d0:d8:37:31,c4:7d:4f:56:e5:19
/Users/enukane/Desktop/emtestpcap//test2.pcapng,1413615217,45,c4:7d:4f:56:e5:19,20:c9:d0:d8:37:31
/Users/enukane/Desktop/emtestpcap//test2.pcapng,1413615217,39,,c4:7d:4f:56:e5:19
(中略)
2015-01-29 11:29:47,517 [INFO]: main:org.embulk.exec.LocalExecutor: {done:  3 / 3, running: 0}
2015-01-29 11:29:47,517 [INFO]: main:org.embulk.exec.LocalExecutor: {done:  3 / 3, running: 0}
2015-01-29 11:29:47,517 [INFO]: main:org.embulk.exec.LocalExecutor: {done:  3 / 3, running: 0}
2015-01-29 11:29:47,535 [INFO]: main:org.embulk.command.Runner: next config: {"type":"pcapng_files","paths":["/Users/enukane/Desktop/emtestpcap/","/tmp"],"threads":3,"schema":[{"name":"frame.time_epoch","type":"long"},{"name":"frame.len","type":"long"},{"name":"wlan.ta","
type":"string"},{"name":"wlan.ra","type":"string"}],"done":["/Users/enukane/Desktop/emtestpcap//test1.pcapng","/Users/enukane/Desktop/emtestpcap//test2.pcapng","/Users/enukane/Desktop/emtestpcap//test5.pcapng"]}
</code></pre>

<ul>
<li>-oオプションで実行中のコンフィグと同じパスを指定してやると, 元のコンフィグに「今処理したファイルリスト」を付け加えます

<ul>
<li>既にdoneが合った場合とか考慮してない等々で今はきちんと動いてない模様&hellip; (to be fixed)</li>
<li>いわゆるcommit reportとして, 次回に重複処理しないように考慮 (したかった)</li>
</ul>
</li>
<li>outがstdoutになっているので, previewの様な表ではなくcsv形式で出力.</li>
</ul>


<h3>embulk弄ってたときのメモ</h3>

<ul>
<li>embulkの<a href="https://github.com/embulk/embulk">README</a>をよめばだいたいどうにかなる。</li>
<li>bundleコマンドで作成したディレクトリ以下でプラグインの新規追加・名前変更等したときは, 再度bundleコマンド発行すること</li>
<li>本来的にはinputプラグインではなくFile input内のparser/decoderプラグインとして造るべきでは？

<ul>
<li>処理対象のファイル一覧→スレッドへの分配あたりを再開発してる感</li>
<li>input/output以外のプラグインがどの程度rubyで差し込みできるのかよく見えない. 要調査.

<ul>
<li>file inputのメインのロジックはJava側で書かれてるようなのでbundle側からのインタラクション次第か?</li>
</ul>
</li>
</ul>
</li>
<li>task to threadsのベストプラクティスが欲しい&hellip;</li>
<li>transactionとrunの間のデータ受け渡し, これでいいのかな感

<ul>
<li>引数でスレッドローカルなオブジェクト渡すものだと思ってたけど&hellip;</li>
</ul>
</li>
</ul>

]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[システム系論文紹介 Advent Calendar 2014]]></title>
    <link href="http://enukane.github.io/blog/2014/12/12/syspaper-adventcalendar-2014/"/>
    <updated>2014-12-12T09:58:24+09:00</updated>
    <id>http://enukane.github.io/blog/2014/12/12/syspaper-adventcalendar-2014</id>
    <content type="html"><![CDATA[<p>本記事は、<a href="http://www.adventar.org/calendars/440">システム系論文紹介 Advent Calendar 2014</a> 12/11 のための記事です</p>

<h1>はじめに</h1>

<p>本エントリでは、NSDI&#8217;14 にて発表のあった &ldquo;NetVM&rdquo; について紹介します。</p>

<h1>&ldquo;NetVM: High Performance and Flexible Networking Using Virtualization on Commodity Platforms&rdquo;</h1>

<h2>論文の概要</h2>

<ul>
<li>タイトル: <a href="https://www.usenix.org/conference/nsdi14/technical-sessions/presentation/hwang"><em>&ldquo;NetVM: High Performance and Flexible Networking Using Virtualization on Commodity Platforms&rdquo;</em></a></li>
<li>著者: Jinho Hwang et al.</li>
<li>出典: 11th USENIX Symposium on Networked Systems Design and Implementation (NSDI&#8217;14)</li>
</ul>


<p>この論文では表題にも登場する&#8221;Commodity Platforms&#8221;いわゆる普通の(?) x86マシンを使った、高度かつ高速なネットワークサービスを実現するための基盤として、&#8221;NetVM&#8221; というプラットフォームを提案&amp;実装しています。これまでお高い専用のハードウェアでなければ実用に足る性能が出せなかったファイヤーウォールやルーティング、ロードバランサといったネットワークに必要な諸機能。これらをソフトウェアだけで実現し、いわゆる<a href="http://www.etsi.org/technologies-clusters/technologies/nfv">NFV</a> 的なものの基盤として使える様にするといったことを目的としています。特にこの論文では、ハードウェアアプライアンスに負けない性能(==ワイヤーレート)を目指しつつ、ソフトウェアとしての展開の容易さや機能実現に対する柔軟性といったネットワークサービスインフラにおける利便性を実現することを主眼としています。</p>

<h1>ここがひと味違うよNetVM</h1>

<p>ここではNetVMを支える技術、として論文中よりNetVMにおける「こだわり」ポイントをいくつか紹介します。</p>

<h2>Virtualization: 仮想化前提だよ</h2>

<h3>NetVMにおける仮想マシン: 1コンポーネント == 1VM</h3>

<p>NetVMにおける一つの特徴として、仮想化環境によるネットワークサービスの実現を特に意識している点が上げられます。
いわゆるハコモノを用いる一般的なネットワークでは用途に応じて様々なハコを組み合わせてネットワークを構成します。
これにはたとえばスイッチやルータ、ファイヤーウォールなどといったコンポーネントに設定を投入し、間を
LANケーブルで敷設するイメージでしょうか。
ブロードバンドルータなど一つのハコにまとめている場合もありますが、より大規模かつ高速な処理を要求される場合は
専用のアプライアンスを持ってくることが一般的でしょう。</p>

<p>NetVMでは、これらの物理的なハコとしての形を取っ払ってKVM上の仮想マシンを新たな仮想的なハコとしてそこに機能を押し込めています。
これによりソフトウェアの柔軟性(デプロイや変更の自動化、大規模化 etc)といった利点をネットワークの構成に導入しています。</p>

<p>それぞれのVMはハコモノとして、これまで行ってきたのと同じようにパケットの入力とそれに基づく出力の決定(宛先の変更やパケットの書き換えなど)を行います。
もちろん仮想マシンに押し込めたところで物理配線が消える訳ではないのでそれらは依然として存在しますが、このハコモノの間を埋めるLANケーブル
(とスイッチング)の役割の一部をNetVMではハイパバイザでも提供します。NetVMではこのホストで動く部分をNetVM Core, NetVM Managerと呼称しています。
ネットワークコンポーネント間の結線にあたるこの部分もソフトウェアとして抱き込むことで、ハコモノを仮想化したのと同様の利点をネットワーキングにも導入しています。</p>

<h2>VMという区分によるセキュリティ</h2>

<p>一つのハコモノを一つのVMに押し込めて動かすことはセキュリティ面でも利点があります。
NFV的な、一つの物理ハードウェア上に複数のネットワーク、複数のお客さん環境が載っている場合は
それぞれの間がまぜこぜにならないことが肝要です。
KVMが既にVMという区分で提供しているアイソレーションの仕組みに乗っかることでNetVMでは
各ネットワークコンポーネントのセキュリティを担保しています。</p>

<p>加えて必要なのは、それぞれのネットワーク間を間違って繋がないことです。
ここはホストOSつまりNetVM CoreやNetVM Managerの領分になりますが、このためにVMのグルーピングができるようになっています。
これにより、あるVM群は特定のユーザのもの、また別のVM群はそっちのユーザのものといった形で内部的に分離する事が可能です。</p>

<p>なおあくまで内部的な区分けにつき、外に出たフレームは対象とはなりません。このため実際にNFVっぽいことの基盤として使うにはその部分でVLANかますなりトンネリングするなりでネットワーク的に区分けすることが必要だったり？また、一つの物理的なハードウェア上での組み合わせを主眼にしているためか外に出ないといけないような場合の想定があまりなされていない感じもあります。</p>

<h2>High Performance: パケット処理性能↑</h2>

<p>ハードウェアなアプライアンス箱に性能で勝つ、というのを第一目標においているためかチューニングやテクニックの記述に論文のそこそこの部分を割いているためその一部をここでは解説します。</p>

<h3>DPDKで User-land Packet Processing</h3>

<p>いわゆる「ポーリングモード」+ 「ユーザランドにおけるパケット処理」+「バッチ処理」による高速化は、Intel DPDKのリリースやnetmap、あとちょっとマニアックなところだとrump(はどうなんだろう&hellip;?)などの登場によりもはや当たり前のテクニックとなってきました。特に&#8221;なんらかのアプリケーション&#8221;に特化した通信を行いたい場合は、これまで先人がせっせと築き上げてきたOSのレイヤやらコンテキストスイッチングによるオーバヘッドをぶち抜けるため非常に高速にパケットを処理することが可能です。</p>

<p>これらの仕組みはDPDKでもnetmapでも、基本的には直接ハードウェア(NIC)を触ってる人のみへの恩恵です。一方、NetVMではベースにDPDKを用いることでこれらの技術を利用しつつ、ネットワークサービスのコンポーネントたるVMの中のアプリケーションに対しても同様の利点を提供しています。</p>

<h3>システム全体でのパケットバッファ共有</h3>

<p>DPDKやnetmapは基本的に、NICとネットワークアプリケーションとで送受信リングとパケットバッファを共有することでコピーコストの削減を行っています。
NetVMでもDPDKを利用しているため、ホストOSとNICとの間でこれはすでになされていますがこの構造をさらに拡張し、VMにたいしても
同様の処理ができるように巨大な共有メモリ領域を用意し、ダミーのvirtual PCIデバイス経由でVMに対してこれを晒しています。
これによりNIC-ホストOS-VMの三者間でのZero Copyを実現しています。
このZero CopyはVM-VM間でも有効であり、ホストOS(NetVM Core)を通してのコンポーネント間のパケットのやりとりにあたっても
同様の恩恵を得ることができます。これは、ホスト-VM間のRx/Txリングがそれぞれ独立である一方で&#8221;同じネットワーク&#8221;間では
パケットバッファが共有されているためです。なお先に述べたネットワーク間のアイソレーション機能(グルーピング)は
このパケットバッファの共有範囲を分けることでも実現されています。</p>

<h3>CPU特性の活用</h3>

<p>バッファ共有によるZero Copyの拡張に加えて、マルチコアシステムを前提として以下の工夫がなされています。</p>

<ul>
<li>Lockless なシステムデザイン</li>
<li>NUMA-awareなデザイン</li>
</ul>


<p>netmapでも同様だったかと思いますが(マルチコアの場合はどうだったっけ&hellip;)、共有のパケットバッファの操作にあたりNetVMではロックを用いません。
あるキュー(Rx/Tx)は同時にホスト内またはゲストから操作しないように構成されます(できないのではなくやらない)。
ある一つのキューに対してロックが必要なケースは以下の場合です。</p>

<ol>
<li>ホストOS側でそれぞれのコアが同じキューを触ることがある場合</li>
<li>ゲストOSとホストOSが同じキューを触ることがある場合</li>
</ol>


<p>これらを避けるために、NetVMではコア間で触るキューを分けるようになっています。
まずNetVMのホストOS側ではコアごとにNICおよびVMとのキューを分けるようになっています。
これにより1の、同じキューを別々のコア(上で動作するスレッド)が触ることがないようにしています。
次に、ゲストOS(VM)側のスレッドとホストOS側のスレッドが同じキューを操作するシチュエーション(2)ですが、
これも1と同様にあるゲストOS(VM)上のスレッドが動くCPUを限定することで対処しています。
このゲスト上のスレッドも、同様に自分のCPUに割り当てられたキューしか触れないようにすることで実質的に
いわゆるキューを挟んだconsumer-producerにおいてRX側では「ホストOSのスレッドとしてキューにpush」と
「VM上のスレッドとしてキューからpop」が一つのCPUで完結することになります(これらは1つのCPU上では同時に実行されないためロックもいらない)。
このあたりの実装にはKVMのvCPU周りの管理機構をいじくって実現したとのこと。</p>

<p>NetVMではNUMAなマルチコアシステムにおけるメモリの距離とキャッシュの扱いについても考慮が入っています。
基本的なデザインとして、NetVMでは自分のコアに距離的に近いメモリ領域以外は触らないように構成されています
(遠いメモリにアクセスするとキャッシュは汚れるは遠いわで良くないよというお話はここでは割愛)。
これは上記の「キューごとのコア」という構造をベースに作られます。
まずNetVMではそれぞれのコアごとに触るメモリ領域を分けます (総メモリ量/コア数 == 1つのコアが触るメモリ領域)。
そしてその各々のメモリ領域に「キューごとのコア」が作られ、ゲスト(VM)と共有されます。
先に述べたように、Locklessデザインの制約によりあるキューはそれぞれ対応するCPUからしかアクセスされないため、
あるCPU(とその上で動くホスト/ゲスト上のスレッド)は常に自分から近いメモリ領域のみを触る様になります。
これは全てのホスト-ゲスト間のみならず、ゲスト-ゲスト間でのパケットのやりとりでも有効であり、最終的に
NICにパケットを引き渡すまでlocalityを維持したまま通信ができるようになります。</p>

<h1>終わりに</h1>

<p>ここではNSDI&#8217;14より&#8221;NetVM&#8221;の紹介をしました。こういったNFV的なプラットフォームはネットワークベンダやキャリアなど各社それぞれ様々な構想を出しており、それらの一例として作り方まで踏み込んだものとして面白い論文ですが、なによりもチューニングに結構命かけているあたりがこのNetVMの面白いところです。</p>

<h2>今回ボツになった他の候補者達</h2>

<p>最後に、今回読もうと思ってた論文の候補たちを並べておきます。</p>

<ol>
<li>Unikernels: Library Operating Systems for the Cloud (ATC&#8217;13)</li>
<li>HACK: Hierarchical ACKs for Efﬁcient Wireless Medium Utilization (ATC&#8217;14)</li>
<li>Hyper-Switch: A Scalable Software Virtual Switching Architecture (ATC&#8217;13)</li>
<li>Rekindling Network Protocol Innovation with User-Level Stacks (SIGCOMM Computer Communication Review)</li>
</ol>


<p>Unikernelsは@suma90h さんが既にシステム系論文輪読会で<a href="http://www.slideshare.net/suma_/reading-unikernels">お読みになってた</a> のでパス。HACKは、無線LAN系 &amp; 今年のATCでのBest Paperということもあって気になっているのですが別枠で読むことに。3や4は、NetVMと同じく&#8221;えすでーえぬ&#8221;とか&#8221;えぬえふぶい&#8221;といった類いに属しそうなのと、先日のIIJ-II セミナーで&#8221;ClickOS and the Art of Network Function Virtualization&#8221;のお話があったので関連文献として上げてみました。</p>
]]></content>
  </entry>
  
</feed>
